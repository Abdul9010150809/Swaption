{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Notebook\n",
    "\n",
    "This notebook demonstrates feature engineering techniques for financial derivative pricing models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import our custom modules\n",
    "from data.data_generator import FinancialDataGenerator\n",
    "from data.feature_engineer import FinancialFeatureEngineer\n",
    "from data.preprocessor import FinancialDataPreprocessor\n",
    "from utils.visualization import FinancialVisualizer\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation and Initial Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "generator = FinancialDataGenerator(seed=42)\n",
    "option_data = generator.generate_option_prices(n_samples=10000)\n",
    "swaption_data = generator.generate_swaption_data(n_samples=5000)\n",
    "\n",
    "print(f\"Option data shape: {option_data.shape}\")\n",
    "print(f\"Swaption data shape: {swaption_data.shape}\")\n",
    "print(f\"\\nOption data columns: {list(option_data.columns)}\")\n",
    "print(f\"Swaption data columns: {list(swaption_data.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Feature Engineering for Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = FinancialFeatureEngineer()\n",
    "\n",
    "# Create option-specific features\n",
    "option_features = feature_engineer.create_option_features(option_data)\n",
    "\n",
    "print(f\"Original features: {option_data.shape[1]}\")\n",
    "print(f\"After feature engineering: {option_features.shape[1]}\")\n",
    "print(f\"\\nNew features added:\")\n",
    "new_features = set(option_features.columns) - set(option_data.columns)\n",
    "for feature in sorted(new_features):\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display feature statistics\n",
    "print(\"Feature Statistics:\")\n",
    "feature_stats = option_features[list(new_features)].describe()\n",
    "print(feature_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Swaption Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create swaption-specific features\n",
    "swaption_features = feature_engineer.create_swaption_features(swaption_data)\n",
    "\n",
    "print(f\"Original swaption features: {swaption_data.shape[1]}\")\n",
    "print(f\"After feature engineering: {swaption_features.shape[1]}\")\n",
    "print(f\"\\nNew swaption features added:\")\n",
    "new_swaption_features = set(swaption_features.columns) - set(swaption_data.columns)\n",
    "for feature in sorted(new_swaption_features):\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Polynomial and Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add polynomial features\n",
    "poly_features = ['volatility', 'time_to_expiry', 'moneyness']\n",
    "option_with_poly = feature_engineer.add_polynomial_features(option_features, poly_features, degree=2)\n",
    "\n",
    "print(f\"After polynomial features: {option_with_poly.shape[1]}\")\n",
    "print(f\"\\nPolynomial features added:\")\n",
    "poly_new_features = set(option_with_poly.columns) - set(option_features.columns)\n",
    "for feature in sorted(poly_new_features):\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add interaction features\n",
    "interaction_pairs = [\n",
    "    ('volatility', 'time_to_expiry'),\n",
    "    ('moneyness', 'volatility'),\n",
    "    ('time_to_expiry', 'risk_free_rate')\n",
    "]\n",
    "\n",
    "option_with_interactions = feature_engineer.add_interaction_features(option_with_poly, interaction_pairs)\n",
    "\n",
    "print(f\"After interaction features: {option_with_interactions.shape[1]}\")\n",
    "print(f\"\\nInteraction features added:\")\n",
    "interaction_new_features = set(option_with_interactions.columns) - set(option_with_poly.columns)\n",
    "for feature in sorted(interaction_new_features):\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling and Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "features_to_scale = ['spot_price', 'strike_price', 'volatility', 'time_to_expiry', 'risk_free_rate']\n",
    "scaled_data = feature_engineer.scale_features(option_with_interactions, features_to_scale, method='standard')\n",
    "\n",
    "print(f\"After scaling: {scaled_data.shape[1]}\")\n",
    "print(f\"\\nScaled features:\")\n",
    "scaled_features = [f\"{feature}_scaled\" for feature in features_to_scale]\n",
    "for feature in scaled_features:\n",
    "    if feature in scaled_data.columns:\n",
    "        print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare original vs scaled distributions\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "fig.suptitle('Original vs Scaled Feature Distributions', fontsize=16)\n",
    "\n",
    "for i, feature in enumerate(features_to_scale[:3]):\n",
    "    # Original distribution\n",
    "    axes[0, i].hist(option_with_interactions[feature], bins=50, alpha=0.7, label='Original')\n",
    "    axes[0, i].set_title(f'Original {feature}')\n",
    "    axes[0, i].set_xlabel(feature)\n",
    "    axes[0, i].set_ylabel('Frequency')\n",
    "    axes[0, i].legend()\n",
    "    \n",
    "    # Scaled distribution\n",
    "    scaled_feature = f\"{feature}_scaled\"\n",
    "    if scaled_feature in scaled_data.columns:\n",
    "        axes[1, i].hist(scaled_data[scaled_feature], bins=50, alpha=0.7, label='Scaled', color='orange')\n",
    "        axes[1, i].set_title(f'Scaled {feature}')\n",
    "        axes[1, i].set_xlabel(f'{feature} (scaled)')\n",
    "        axes[1, i].set_ylabel('Frequency')\n",
    "        axes[1, i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dimensionality Reduction with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select numerical features for PCA\n",
    "numerical_features = scaled_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_features = [f for f in numerical_features if not f.endswith('_scaled')]  # Exclude already scaled\n",
    "\n",
    "# Remove target variables\n",
    "features_for_pca = [f for f in numerical_features if f not in ['call_price', 'put_price']]\n",
    "\n",
    "print(f\"Features for PCA: {len(features_for_pca)}\")\n",
    "print(f\"Features: {features_for_pca[:10]}...\")  # Show first 10\n",
    "\n",
    "# Apply PCA\n",
    "pca_data = feature_engineer.apply_pca(scaled_data, features_for_pca, n_components=10)\n",
    "\n",
    "print(f\"\\nAfter PCA: {pca_data.shape[1]}\")\n",
    "print(f\"PCA components added: {[col for col in pca_data.columns if col.startswith('pca_component_')]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize PCA results\n",
    "pca_cols = [col for col in pca_data.columns if col.startswith('pca_component_')]\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "\n",
    "# Plot explained variance\n",
    "plt.subplot(1, 2, 1)\n",
    "explained_variance = feature_engineer.pca_models['pca'].explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "plt.bar(range(1, len(explained_variance) + 1), explained_variance, alpha=0.7, label='Individual')\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'ro-', label='Cumulative')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot first two components\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(pca_data['pca_component_1'], pca_data['pca_component_2'], alpha=0.6, s=1)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.title('PCA Components Scatter Plot')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for feature selection\n",
    "X = pca_data.drop(['call_price', 'put_price'], axis=1)\n",
    "y = pca_data['call_price']\n",
    "\n",
    "# Remove non-numerical columns\n",
    "X_numerical = X.select_dtypes(include=[np.number])\n",
    "\n",
    "print(f\"Features for selection: {X_numerical.shape[1]}\")\n",
    "\n",
    "# Select top k features\n",
    "selector = SelectKBest(score_func=f_regression, k=20)\n",
    "X_selected = selector.fit_transform(X_numerical, y)\n",
    "\n",
    "# Get selected feature names\n",
    "selected_mask = selector.get_support()\n",
    "selected_features = X_numerical.columns[selected_mask].tolist()\n",
    "\n",
    "print(f\"\\nTop 20 selected features:\")\n",
    "for i, feature in enumerate(selected_features, 1):\n",
    "    score = selector.scores_[X_numerical.columns.get_loc(feature)]\n",
    "    print(f\"{i:2d}. {feature:<30} Score: {score:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Get all feature scores\n",
    "all_scores = selector.scores_\n",
    "all_features = X_numerical.columns\n",
    "\n",
    "# Sort by score\n",
    "sorted_indices = np.argsort(all_scores)[::-1]\n",
    "top_features = all_features[sorted_indices][:20]\n",
    "top_scores = all_scores[sorted_indices][:20]\n",
    "\n",
    "plt.barh(range(len(top_features)), top_scores)\n",
    "plt.yticks(range(len(top_features)), top_features)\n",
    "plt.xlabel('F-Statistic Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Top 20 Feature Importance Scores')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Rolling Statistics and Time-Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time-based features (simulate time series)\n",
    "# Add a time index to simulate temporal data\n",
    "option_with_time = option_features.copy()\n",
    "option_with_time['time_index'] = range(len(option_with_time))\n",
    "option_with_time = option_with_time.sort_values('time_index')\n",
    "\n",
    "# Create rolling features\n",
    "rolling_features = feature_engineer.create_rolling_features(\n",
    "    option_with_time, \n",
    "    ['volatility', 'call_price'], \n",
    "    windows=[5, 10, 20]\n",
    ")\n",
    "\n",
    "print(f\"After rolling features: {rolling_features.shape[1]}\")\n",
    "print(f\"\\nRolling features added:\")\n",
    "rolling_new_features = set(rolling_features.columns) - set(option_with_time.columns)\n",
    "for feature in sorted(rolling_new_features):\n",
    "    print(f\"  - {feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Complete Feature Engineering Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_complete_feature_set(data, target_column='call_price', n_pca_components=10, n_select_features=20):\n",
    "    \"\"\"\n",
    "    Complete feature engineering pipeline\n",
    "    \"\"\"\n",
    "    print(\"Starting complete feature engineering pipeline...\")\n",
    "    \n",
    "    # Initialize feature engineer\n",
    "    engineer = FinancialFeatureEngineer()\n",
    "    \n",
    "    # Step 1: Domain-specific features\n",
    "    if 'strike_price' in data.columns:  # Option data\n",
    "        data = engineer.create_option_features(data)\n",
    "        print(f\"✓ Created option-specific features: {data.shape[1]} total features\")\n",
    "    elif 'swap_rate' in data.columns:  # Swaption data\n",
    "        data = engineer.create_swaption_features(data)\n",
    "        print(f\"✓ Created swaption-specific features: {data.shape[1]} total features\")\n",
    "    \n",
    "    # Step 2: Polynomial features\n",
    "    poly_features = ['volatility', 'time_to_expiry']\n",
    "    if 'moneyness' in data.columns:\n",
    "        poly_features.append('moneyness')\n",
    "    \n",
    "    available_poly_features = [f for f in poly_features if f in data.columns]\n",
    "    data = engineer.add_polynomial_features(data, available_poly_features, degree=2)\n",
    "    print(f\"✓ Added polynomial features: {data.shape[1]} total features\")\n",
    "    \n",
    "    # Step 3: Interaction features\n",
    "    interaction_pairs = [\n",
    "        ('volatility', 'time_to_expiry'),\n",
    "        ('volatility', 'risk_free_rate')\n",
    "    ]\n",
    "    available_pairs = [(f1, f2) for f1, f2 in interaction_pairs \n",
    "                      if f1 in data.columns and f2 in data.columns]\n",
    "    data = engineer.add_interaction_features(data, available_pairs)\n",
    "    print(f\"✓ Added interaction features: {data.shape[1]} total features\")\n",
    "    \n",
    "    # Step 4: Scaling\n",
    "    features_to_scale = ['spot_price', 'strike_price', 'volatility', 'time_to_expiry', 'risk_free_rate']\n",
    "    available_scale_features = [f for f in features_to_scale if f in data.columns]\n",
    "    data = engineer.scale_features(data, available_scale_features, method='standard')\n",
    "    print(f\"✓ Scaled features: {data.shape[1]} total features\")\n",
    "    \n",
    "    # Step 5: PCA\n",
    "    numerical_features = data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    pca_features = [f for f in numerical_features \n",
    "                   if not f.endswith('_scaled') and f != target_column and f in data.columns]\n",
    "    \n",
    "    data = engineer.apply_pca(data, pca_features, n_components=n_pca_components)\n",
    "    print(f\"✓ Applied PCA: {data.shape[1]} total features\")\n",
    "    \n",
    "    # Step 6: Feature selection\n",
    "    X = data.drop([target_column], axis=1, errors='ignore')\n",
    "    y = data[target_column] if target_column in data.columns else None\n",
    "    \n",
    "    if y is not None:\n",
    "        X_numerical = X.select_dtypes(include=[np.number])\n",
    "        \n",
    "        selector = SelectKBest(score_func=f_regression, k=min(n_select_features, X_numerical.shape[1]))\n",
    "        X_selected = selector.fit_transform(X_numerical, y)\n",
    "        \n",
    "        selected_mask = selector.get_support()\n",
    "        selected_feature_names = X_numerical.columns[selected_mask].tolist()\n",
    "        \n",
    "        # Keep only selected features plus target\n",
    "        keep_columns = selected_feature_names + ([target_column] if target_column in data.columns else [])\n",
    "        data = data[keep_columns]\n",
    "        print(f\"✓ Selected top features: {data.shape[1]} total features\")\n",
    "    \n",
    "    print(f\"\\nFeature engineering complete!\")\n",
    "    print(f\"Final dataset shape: {data.shape}\")\n",
    "    \n",
    "    return data, engineer\n",
    "\n",
    "# Test the complete pipeline\n",
    "sample_data = option_data.head(1000).copy()  # Use smaller sample for testing\n",
    "processed_data, engineer = create_complete_feature_set(sample_data)\n",
    "\n",
    "print(f\"\\nProcessed data columns:\")\n",
    "for i, col in enumerate(processed_data.columns, 1):\n",
    "    print(f\"{i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Feature Engineering Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== FEATURE ENGINEERING SUMMARY ===\")\n",
    "print(f\"Original option features: {len(option_data.columns)}\")\n",
    "print(f\"After domain-specific features: {len(option_features.columns)}\")\n",
    "print(f\"After polynomial features: {len(option_with_poly.columns)}\")\n",
    "print(f\"After interaction features: {len(option_with_interactions.columns)}\")\n",
    "print(f\"After scaling: {len(scaled_data.columns)}\")\n",
    "print(f\"After PCA: {len(pca_data.columns)}\")\n",
    "print(f\"After feature selection: {len(processed_data.columns)}\")\n",
    "print()\n",
    "\n",
    "print(\"FEATURE TYPES CREATED:\")\n",
    "print(\"1. Domain-specific features (moneyness, time-value, etc.)\")\n",
    "print(\"2. Polynomial features (squared terms, etc.)\")\n",
    "print(\"3. Interaction features (feature products)\")\n",
    "print(\"4. Scaled features (standardized)\")\n",
    "print(\"5. PCA components (dimensionality reduction)\")\n",
    "print(\"6. Selected features (most important)\")\n",
    "print()\n",
    "\n",
    "print(\"KEY BENEFITS:\")\n",
    "print(\"- Captures non-linear relationships\")\n",
    "print(\"- Reduces dimensionality while preserving information\")\n",
    "print(\"- Improves model interpretability\")\n",
    "print(\"- Handles multicollinearity\")\n",
    "print(\"- Enables better generalization\")\n",
    "print()\n",
    "\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"1. Model training with engineered features\")\n",
    "print(\"2. Cross-validation to prevent overfitting\")\n",
    "print(\"3. Feature importance analysis\")\n",
    "print(\"4. Model interpretation and validation\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}