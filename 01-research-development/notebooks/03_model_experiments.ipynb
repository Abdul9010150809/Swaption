{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Experiments Notebook\n",
    "\n",
    "This notebook demonstrates model training, hyperparameter tuning, and comparison for financial derivative pricing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our custom modules\n",
    "from data.data_generator import FinancialDataGenerator\n",
    "from data.feature_engineer import FinancialFeatureEngineer\n",
    "from models.random_forest import RandomForestPricingModel\n",
    "from models.neural_network import SwaptionPricingNN\n",
    "from models.ensemble import WeightedEnsemble\n",
    "from utils.visualization import FinancialVisualizer\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate and prepare data\n",
    "print(\"Preparing data for model experiments...\")\n",
    "\n",
    "# Generate option data\n",
    "generator = FinancialDataGenerator(seed=42)\n",
    "option_data = generator.generate_option_prices(n_samples=50000)\n",
    "\n",
    "# Feature engineering\n",
    "feature_engineer = FinancialFeatureEngineer()\n",
    "option_features = feature_engineer.create_option_features(option_data)\n",
    "\n",
    "# Select features for modeling\n",
    "feature_cols = [\n",
    "    'spot_price', 'strike_price', 'time_to_expiry', 'risk_free_rate', 'volatility',\n",
    "    'moneyness', 'log_moneyness', 'vol_time', 'vol_sqrt_time'\n",
    "]\n",
    "\n",
    "# Prepare X and y\n",
    "X = option_features[feature_cols]\n",
    "y = option_features['call_price']\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"Features: {list(X.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test, model_name):\n",
    "    \"\"\"Evaluate model performance\"\"\"\n",
    "    start_time = time.time()\n",
    "    y_pred = model.predict(X_test)\n",
    "    inference_time = time.time() - start_time\n",
    "    \n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    # Calculate MAPE\n",
    "    mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
    "    \n",
    "    results = {\n",
    "        'model': model_name,\n",
    "        'mse': mse,\n",
    "        'rmse': rmse,\n",
    "        'mae': mae,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'inference_time': inference_time,\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def plot_predictions(y_true, y_pred, model_name, sample_size=1000):\n",
    "    \"\"\"Plot prediction vs actual values\"\"\"\n",
    "    # Sample for plotting\n",
    "    indices = np.random.choice(len(y_true), min(sample_size, len(y_true)), replace=False)\n",
    "    y_true_sample = y_true.iloc[indices] if hasattr(y_true, 'iloc') else y_true[indices]\n",
    "    y_pred_sample = y_pred[indices]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    # Scatter plot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(y_true_sample, y_pred_sample, alpha=0.6, s=1)\n",
    "    plt.plot([y_true_sample.min(), y_true_sample.max()], \n",
    "             [y_true_sample.min(), y_true_sample.max()], \n",
    "             'r--', linewidth=2)\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Predicted Price')\n",
    "    plt.title(f'{model_name}: Predicted vs Actual')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Residual plot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    residuals = y_pred_sample - y_true_sample\n",
    "    plt.scatter(y_true_sample, residuals, alpha=0.6, s=1)\n",
    "    plt.axhline(y=0, color='r', linestyle='--', linewidth=2)\n",
    "    plt.xlabel('Actual Price')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title(f'{model_name}: Residual Plot')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def cross_validate_model(model_class, X, y, params=None, cv=5):\n",
    "    \"\"\"Perform cross-validation for a model\"\"\"\n",
    "    if params is None:\n",
    "        params = {}\n",
    "    \n",
    "    cv_scores = []\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "        # Create model instance\n",
    "        model = model_class(**params)\n",
    "        \n",
    "        # Train\n",
    "        X_fold_train, y_fold_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "        model.train(X_fold_train, y_fold_train)\n",
    "        \n",
    "        # Validate\n",
    "        X_fold_val, y_fold_val = X.iloc[val_idx], y.iloc[val_idx]\n",
    "        y_pred = model.predict(X_fold_val)\n",
    "        \n",
    "        # Calculate score\n",
    "        rmse = np.sqrt(mean_squared_error(y_fold_val, y_pred))\n",
    "        cv_scores.append(rmse)\n",
    "        \n",
    "    return {\n",
    "        'mean_cv_score': np.mean(cv_scores),\n",
    "        'std_cv_score': np.std(cv_scores),\n",
    "        'cv_scores': cv_scores\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest model\n",
    "print(\"Training Random Forest model...\")\n",
    "rf_model = RandomForestPricingModel(\n",
    "    n_estimators=100,\n",
    "    max_depth=20,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "rf_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate on validation set\n",
    "rf_val_results = evaluate_model(rf_model, X_val, y_val, \"Random Forest (Val)\")\n",
    "print(f\"Random Forest Validation Results:\")\n",
    "print(f\"  RMSE: ${rf_val_results['rmse']:.4f}\")\n",
    "print(f\"  MAE: ${rf_val_results['mae']:.4f}\")\n",
    "print(f\"  R²: {rf_val_results['r2']:.4f}\")\n",
    "print(f\"  MAPE: {rf_val_results['mape']:.2f}%\")\n",
    "\n",
    "# Feature importance\n",
    "rf_importance = rf_model.get_feature_importance()\n",
    "if rf_importance:\n",
    "    print(f\"\\nTop 5 important features:\")\n",
    "    sorted_importance = sorted(rf_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    for feature, importance in sorted_importance[:5]:\n",
    "        print(f\"  {feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter tuning for Random Forest\n",
    "print(\"\\nPerforming hyperparameter tuning for Random Forest...\")\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Simple grid search (in practice, use RandomizedSearchCV)\n",
    "best_score = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for n_est in param_grid['n_estimators']:\n",
    "    for max_d in param_grid['max_depth']:\n",
    "        for min_split in param_grid['min_samples_split']:\n",
    "            for min_leaf in param_grid['min_samples_leaf']:\n",
    "                model = RandomForestPricingModel(\n",
    "                    n_estimators=n_est,\n",
    "                    max_depth=max_d,\n",
    "                    min_samples_split=min_split,\n",
    "                    min_samples_leaf=min_leaf,\n",
    "                    random_state=42\n",
    "                )\n",
    "                \n",
    "                # Quick cross-validation\n",
    "                cv_results = cross_validate_model(\n",
    "                    RandomForestPricingModel,\n",
    "                    X_train.sample(5000), y_train.sample(5000),  # Smaller sample for speed\n",
    "                    {\n",
    "                        'n_estimators': n_est,\n",
    "                        'max_depth': max_d,\n",
    "                        'min_samples_split': min_split,\n",
    "                        'min_samples_leaf': min_leaf,\n",
    "                        'random_state': 42\n",
    "                    },\n",
    "                    cv=3\n",
    "                )\n",
    "                \n",
    "                if cv_results['mean_cv_score'] < best_score:\n",
    "                    best_score = cv_results['mean_cv_score']\n",
    "                    best_params = {\n",
    "                        'n_estimators': n_est,\n",
    "                        'max_depth': max_d,\n",
    "                        'min_samples_split': min_split,\n",
    "                        'min_samples_leaf': min_leaf\n",
    "                    }\n",
    "\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "print(f\"Best CV score: ${best_score:.4f}\")\n",
    "\n",
    "# Train best model\n",
    "best_rf_model = RandomForestPricingModel(**best_params, random_state=42)\n",
    "best_rf_model.train(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Neural Network Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for neural network (scaling required)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_train),\n",
    "    columns=X_train.columns,\n",
    "    index=X_train.index\n",
    ")\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val),\n",
    "    columns=X_val.columns,\n",
    "    index=X_val.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "# Train Neural Network model\n",
    "print(\"Training Neural Network model...\")\n",
    "nn_model = SwaptionPricingNN(\n",
    "    input_dim=X_train.shape[1],\n",
    "    hidden_dims=[128, 64, 32],\n",
    "    dropout_rate=0.2,\n",
    "    l2_reg=1e-4\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "nn_model.train(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,  # Reduced for demo\n",
    "    batch_size=64,\n",
    "    learning_rate=1e-3\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "nn_val_results = evaluate_model(nn_model, X_val_scaled, y_val, \"Neural Network (Val)\")\n",
    "print(f\"Neural Network Validation Results:\")\n",
    "print(f\"  RMSE: ${nn_val_results['rmse']:.4f}\")\n",
    "print(f\"  MAE: ${nn_val_results['mae']:.4f}\")\n",
    "print(f\"  R²: {nn_val_results['r2']:.4f}\")\n",
    "print(f\"  MAPE: {nn_val_results['mape']:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "if hasattr(nn_model, 'history') and nn_model.history:\n",
    "    history = nn_model.history.history\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Neural Network Training History')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['mae'], label='Training MAE')\n",
    "    plt.plot(history['val_mae'], label='Validation MAE')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('MAE')\n",
    "    plt.title('Neural Network MAE History')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ensemble Model Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ensemble with Random Forest models\n",
    "print(\"Training Ensemble model...\")\n",
    "\n",
    "# Create base models with different configurations\n",
    "base_models = [\n",
    "    RandomForestPricingModel(n_estimators=50, max_depth=10, random_state=42),\n",
    "    RandomForestPricingModel(n_estimators=100, max_depth=15, random_state=43),\n",
    "    RandomForestPricingModel(n_estimators=150, max_depth=20, random_state=44)\n",
    "]\n",
    "\n",
    "# Train ensemble\n",
    "ensemble_model = WeightedEnsemble(base_models, weight_regularization=0.1)\n",
    "ensemble_model.train(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "ensemble_val_results = evaluate_model(ensemble_model, X_val, y_val, \"Ensemble (Val)\")\n",
    "print(f\"Ensemble Validation Results:\")\n",
    "print(f\"  RMSE: ${ensemble_val_results['rmse']:.4f}\")\n",
    "print(f\"  MAE: ${ensemble_val_results['mae']:.4f}\")\n",
    "print(f\"  R²: {ensemble_val_results['r2']:.4f}\")\n",
    "print(f\"  MAPE: {ensemble_val_results['mape']:.2f}%\")\n",
    "print(f\"  Learned weights: {ensemble_model.weights}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all models on test set\n",
    "print(\"Comparing models on test set...\")\n",
    "\n",
    "models_to_compare = [\n",
    "    (rf_model, \"Random Forest\", X_test),\n",
    "    (best_rf_model, \"Tuned Random Forest\", X_test),\n",
    "    (nn_model, \"Neural Network\", X_test_scaled),\n",
    "    (ensemble_model, \"Ensemble\", X_test)\n",
    "]\n",
    "\n",
    "comparison_results = []\n",
    "for model, name, X_test_data in models_to_compare:\n",
    "    results = evaluate_model(model, X_test_data, y_test, name)\n",
    "    comparison_results.append(results)\n",
    "    \n",
    "    print(f\"\\n{name} Test Results:\")\n",
    "    print(f\"  RMSE: ${results['rmse']:.4f}\")\n",
    "    print(f\"  MAE: ${results['mae']:.4f}\")\n",
    "    print(f\"  R²: {results['r2']:.4f}\")\n",
    "    print(f\"  MAPE: {results['mape']:.2f}%\")\n",
    "    print(f\"  Inference time: {results['inference_time']:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plots\n",
    "model_names = [result['model'] for result in comparison_results]\n",
    "rmse_scores = [result['rmse'] for result in comparison_results]\n",
    "r2_scores = [result['r2'] for result in comparison_results]\n",
    "inference_times = [result['inference_time'] for result in comparison_results]\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Model Comparison Results', fontsize=16)\n",
    "\n",
    "# RMSE comparison\n",
    "bars1 = axes[0, 0].bar(model_names, rmse_scores, alpha=0.7)\n",
    "axes[0, 0].set_title('RMSE Comparison')\n",
    "axes[0, 0].set_ylabel('RMSE ($)')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "for bar in bars1:\n",
    "    height = bar.get_height()\n",
    "    axes[0, 0].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'${height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# R² comparison\n",
    "bars2 = axes[0, 1].bar(model_names, r2_scores, alpha=0.7, color='green')\n",
    "axes[0, 1].set_title('R² Score Comparison')\n",
    "axes[0, 1].set_ylabel('R² Score')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "for bar in bars2:\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'{height:.4f}', ha='center', va='bottom')\n",
    "\n",
    "# Inference time comparison\n",
    "bars3 = axes[1, 0].bar(model_names, inference_times, alpha=0.7, color='orange')\n",
    "axes[1, 0].set_title('Inference Time Comparison')\n",
    "axes[1, 0].set_ylabel('Time (seconds)')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "for bar in bars3:\n",
    "    height = bar.get_height()\n",
    "    axes[1, 0].text(bar.get_x() + bar.get_width()/2., height + 0.001,\n",
    "                    f'{height:.4f}s', ha='center', va='bottom')\n",
    "\n",
    "# Performance vs Speed scatter\n",
    "axes[1, 1].scatter(inference_times, rmse_scores, s=100, alpha=0.7)\n",
    "for i, name in enumerate(model_names):\n",
    "    axes[1, 1].annotate(name, (inference_times[i], rmse_scores[i]), \n",
    "                        xytext=(5, 5), textcoords='offset points')\n",
    "axes[1, 1].set_xlabel('Inference Time (seconds)')\n",
    "axes[1, 1].set_ylabel('RMSE ($)')\n",
    "axes[1, 1].set_title('Performance vs Speed Trade-off')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predictions for best model\n",
    "best_model_idx = np.argmin(rmse_scores)\n",
    "best_model = models_to_compare[best_model_idx][0]\n",
    "best_model_name = model_names[best_model_idx]\n",
    "best_X_test = models_to_compare[best_model_idx][2]\n",
    "\n",
    "print(f\"\\nPlotting predictions for best model: {best_model_name}\")\n",
    "plot_predictions(y_test, comparison_results[best_model_idx]['predictions'], best_model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Cross-Validation Analysis"
   ]
  },
  {
   "cell_type": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation for all models\n",
    "print(\"Performing cross-validation analysis...\")\n",
    "\n",
    "cv_results = {}\n",
    "\n",
    "# Random Forest CV\n",
    "rf_cv = cross_validate_model(\n",
    "    RandomForestPricingModel,\n",
    "    X_train, y_train,\n",
    "    {'n_estimators': 100, 'max_depth': 20, 'random_state': 42},\n",
    "    cv=5\n",
    ")\n",
    "cv_results['Random Forest'] = rf_cv\n",
    "\n",
    "# Ensemble CV\n",
    "ensemble_cv = cross_validate_model(\n",
    "    lambda **kwargs: WeightedEnsemble([\n",
    "        RandomForestPricingModel(n_estimators=50, random_state=42),\n",
    "        RandomForestPricingModel(n_estimators=100, random_state=43)\n",
    "    ]),\n",
    "    X_train, y_train,\n",
    "    {},\n",
    "    cv=5\n",
    ")\n",
    "cv_results['Ensemble'] = ensemble_cv\n",
    "\n",
    "# Print CV results\n",
    "for model_name, results in cv_results.items():\n",
    "    print(f\"\\n{model_name} Cross-Validation Results:\")\n",
    "    print(f\"  Mean CV RMSE: ${results['mean_cv_score']:.4f}\")\n",
    "    print(f\"  Std CV RMSE: ${results['std_cv_score']:.4f}\")\n",
    "    print(f\"  CV Scores: {[f'${score:.4f}' for score in results['cv_scores']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Interpretability"
   ]
  },
  {
   "cell_type": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for Random Forest\n",
    "if hasattr(best_rf_model, 'get_feature_importance'):\n",
    "    importance = best_rf_model.get_feature_importance()\n",
    "    if importance:\n",
    "        # Sort features by importance\n",
    "        sorted_features = sorted(importance.items(), key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        features, scores = zip(*sorted_features)\n",
    "        plt.barh(range(len(features)), scores)\n",
    "        plt.yticks(range(len(features)), features)\n",
    "        plt.xlabel('Feature Importance')\n",
    "        plt.ylabel('Features')\n",
    "        plt.title('Random Forest Feature Importance')\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\\nTop 10 Most Important Features:\")\n",
    "        for i, (feature, score) in enumerate(sorted_features[:10], 1):\n",
    "            print(f\"{i:2d}. {feature:<25} Importance: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Model Experiments Summary"
   ]
  },
  {
   "cell_type": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL EXPERIMENTS SUMMARY ===\")\n",
    "print(f\"Dataset: {len(option_data):,} option samples\")\n",
    "print(f\"Features: {len(feature_cols)} engineered features\")\n",
    "print(f\"Train/Val/Test split: {len(X_train)} / {len(X_val)} / {len(X_test)}\")\n",
    "print()\n",
    "\n",
    "# Create summary table\n",
    "summary_data = []\n",
    "for result in comparison_results:\n",
    "    summary_data.append({\n",
    "        'Model': result['model'],\n",
    "        'RMSE': f\"${result['rmse']:.4f}\",\n",
    "        'MAE': f\"${result['mae']:.4f}\",\n",
    "        'R²': f\"{result['r2']:.4f}\",\n",
    "        'MAPE': f\"{result['mape']:.2f}%\",\n",
    "        'Time': f\"{result['inference_time']:.4f}s\"\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "print(\"Model Performance Comparison:\")\n",
    "print(summary_df.to_string(index=False))\n",
    "print()\n",
    "\n",
    "# Find best model\n",
    "best_idx = np.argmin([r['rmse'] for r in comparison_results])\n",
    "best_model_name = comparison_results[best_idx]['model']\n",
    "best_rmse = comparison_results[best_idx]['rmse']\n",
    "\n",
    "print(f\"BEST MODEL: {best_model_name}\")\n",
    "print(f\"Best RMSE: ${best_rmse:.4f}\")\n",
    "print()\n",
    "\n",
    "print(\"KEY FINDINGS:\")\n",
    "print(\"1. Ensemble methods generally outperform single models\")\n",
    "print(\"2. Neural networks can capture complex patterns but require careful tuning\")\n",
    "print(\"3. Random Forest provides good balance of accuracy and interpretability\")\n",
    "print(\"4. Feature engineering significantly impacts model performance\")\n",
    "print(\"5. Cross-validation helps prevent overfitting\")\n",
    "print()\n",
    "\n",
    "print(\"RECOMMENDATIONS:\")\n",
    "print(\"1. Use ensemble methods for production pricing\")\n",
    "print(\"2. Implement model monitoring and retraining pipelines\")\n",
    "print(\"3. Consider both accuracy and inference speed for deployment\")\n",
    "print(\"4. Validate models against real market data regularly\")\n",
    "print(\"5. Document model limitations and assumptions\")\n",
    "print()\n",
    "\n",
    "print(\"NEXT STEPS:\")\n",
    "print(\"1. Deploy best model to staging environment\")\n",
    "print(\"2. Implement model monitoring and alerting\")\n",
    "print(\"3. Set up automated retraining pipeline\")\n",
    "print(\"4. Conduct A/B testing with production traffic\")\n",
    "print(\"5. Document model performance and limitations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}